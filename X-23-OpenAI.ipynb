{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d3a007",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"images/inmas.png\" width=130x align='right' />\n",
    "\n",
    "# Exercises 23 - OpenAI Application Programming Interface\n",
    "    \n",
    "This notebook requires that you concurrently use a terminal. Use virtual desktops if possible (e.g., Windows Key+Ctl+arrows).\n",
    "\n",
    "### Prerequisite\n",
    "Notebook 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bded63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave, struct, os\n",
    "from openai import OpenAI                        # The API to OpenAI\n",
    "from pvrecorder import PvRecorder                # To record our voice\n",
    "from playsound import playsound                  # To play the generated speech\n",
    "from IPython.display import Image, display       # To show generated images\n",
    "\n",
    "# To silence deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c23150",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Experiment with different voices and cues for the personality of the system and for the image generation in the last version of the ChatBot class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9a6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c24804bd",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Use a call to 'gpt-4o' to generate code in python to play Hangman. Copy the response message to a cell and run the program to see if it behaves as it should. You will need an API key and some credits on OpenAI to accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af7f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4098cea4",
   "metadata": {},
   "source": [
    "### 3.*\n",
    "This exercise further develops the class ChatBot that was presented in the Notebook. We reproduce it here for your convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ad2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    '''A class to interact with OpenAI API and keep track of context.'''\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = [{'role': 'system', 'content': 'You are math professor with a dark sense of humor.'}]\n",
    "\n",
    "    def chat(self, question):\n",
    "        self.context.append({'role': 'user', 'content': question})\n",
    "        response = self.client.chat.completions.create(model=self.model, messages=self.context)\n",
    "        response_content = response.choices[0].message.content\n",
    "        self.context.append({'role': 'assistant', 'content': response_content})\n",
    "        self.print_chat()\n",
    "        self.speak(response_content, len(self.context)/2)\n",
    "\n",
    "    def speak(self, message, index=0):\n",
    "        speech_file = os.getcwd() + '\\\\_speech_%03d.mp3'%index\n",
    "        response = client.audio.speech.create(model='tts-1-hd', voice='echo', input=message)\n",
    "        if os.path.exists(speech_file):\n",
    "            os.remove(speech_file)\n",
    "        response.stream_to_file(speech_file)\n",
    "        playsound(speech_file)\n",
    "        \n",
    "    def print_chat(self):\n",
    "        for message in self.context:\n",
    "            if message['role'] == 'user':\n",
    "                print('USER: %s' % message['content'])\n",
    "            elif message['role'] == 'assistant':\n",
    "                print('BOT: %s' % message['content'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef977a4",
   "metadata": {},
   "source": [
    "We would like to speak to our Bot and convert our speech to text using the *Whisper* model in OpenAI. \n",
    "\n",
    "The first step is to record our voice using the following code. It records until a kernel interrupt is sent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index = 0\n",
    "audiofile = '_audio_recording%03d.wav'%index\n",
    "recorder = PvRecorder(device_index=-1, frame_length=512)\n",
    "audio = []\n",
    "\n",
    "recorder.start()\n",
    "print('Audio recording started ... you have 10 seconds:')\n",
    "t_end = time.time() + 10\n",
    "while time.time() < t_end:\n",
    "        frame = recorder.read()\n",
    "        audio.extend(frame)\n",
    "finally:\n",
    "        recorder.delete()\n",
    "\n",
    "recorder.stop()\n",
    "print('Audio recording stopped ...')\n",
    "with wave.open(audiofile, 'w') as f:\n",
    "    f.setparams((1, 2, 1600, 512, 'NONE', 'NONE'))\n",
    "    f.writeframes(struct.pack('h'*len(audio), *audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed8970",
   "metadata": {},
   "source": [
    "We now use the audiofile which contains our recorded voice and pass it to *Whisper* for transcription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(audiofile) as f:\n",
    "    transcript = client.audio.transcription.create(model='whisper-1', file=f)\n",
    "\n",
    "transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca9afb",
   "metadata": {},
   "source": [
    "Using the content of the last cell for a method called `transcribe(self, audiofile)` which returns the transcribed text to be passed as a message, and the content of the previous one in a method called `record_audio(self, index=0)` which return the path of the audio file,  integrate the last two cells in the ChatBot class. One additional method should be added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ddfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voicechat(self):\n",
    "    recorded_audio = self.record_audio(index=len(self.context)/2)   # We use the response count as a numbered unique filename\n",
    "    message = self.transcribe(recorded_audio)\n",
    "    self.chat(message)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
