{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d3a007",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/inmas.png\" width=130x align='right' />\n",
    "\n",
    "# Notebook 23 - OpenAI\n",
    "This notebook is a fun tutorial on using the API of OpenAI. It covers:\n",
    "\n",
    "- Using the OpenAI API\n",
    "- Using chat completions\n",
    "- Speech-to-text and text-to-speech generation\n",
    "- Image generation\n",
    "\n",
    "### Prerequisite\n",
    "Notebook 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b928a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This notebook relies on the *chatgpt* environment installed in previous notebook\n",
    "The environment *chatgpt* should have been created as part of the first exercise in the *Virtual Environments* notebook which is reproduced here:\n",
    "\n",
    "#### Exercise 1.\n",
    "<small>\n",
    "This exercise will create the environment needed for the next notebook on using the OpenAI API.\n",
    "\n",
    "Using a terminal and the command line, create a new environment called chatgpt:\n",
    "\n",
    "- Make sure that the environment is visible in Jupyter\n",
    "- Add the following packages: openai, playsound, and pvrecorder\n",
    "- Test if you can load these modules from an empty Jupyter Notebook\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6c1e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Enabling the right kernel\n",
    "- To enable the *chatgpt* environment in Jupyter:\n",
    "    - Select *Kernel -> Change Kernel -> Python [conda env:chatgpt]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e259b6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Loading the required modules\n",
    "The following modules should be available from the *chatgpt* virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26aacd",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI                        # The API to OpenAI\n",
    "from playsound import playsound                  # To play the generated speech\n",
    "from IPython.display import Image, display       # To show generated images\n",
    "\n",
    "# To silence deprecation warnings of streaming_response\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654872e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Obtaining credits from OpenAI\n",
    "For successfully running this notebook, we either need to open a new account on OpenAI, and buy some credits\n",
    "\n",
    "- If you haven't already, sign up on [platform.openai.com](https://platform.openai.com)\n",
    "- Go to billing and buy \\$5 of non-renewable credits - do not select auto-refill\n",
    "\n",
    "### The OpenAI API requires an account and some credits to be able to make requests to the platform\n",
    "- You will not be able to run this notebook successfully without credits and an account on openAI\n",
    "- Each query costs about a penny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c4e10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Obtaining an API key\n",
    "Before we can use the API of chatGPT, we need to obtain a key from OpenAI\n",
    "\n",
    "\n",
    "- Go on *Quickstart* to *Create and export API key*\n",
    "- Give a name to your key, say 'myFirstKey'\n",
    "- Once generated, hit Copy to copy the key and enter it in the cell below by pasting\n",
    "- Save the key as there is no way to see it again on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ce380",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "APIkey = 'YOUR_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26528b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generating a client object with our newly-obtained API key\n",
    "We are now ready to create a client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcc5c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=APIkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba196f85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can now directly interact with OpenAI through this `client` object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6a014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OpenAI offers different API's and models\n",
    "\n",
    "\n",
    "They include:\n",
    "- Many models for chat completion - GPT\n",
    "    - gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo, etc.\n",
    "- Image generation from text - DALL-E\n",
    "    - dall-e-3, dall-e-2\n",
    "- Text conversion to spoken text - TTS (text to speech)\n",
    "    - tts-1\n",
    "- Audio to text translation - Whisper\n",
    "    - whisper-1\n",
    "\n",
    "A list of all the models contained in the API can be found [here](https://platform.openai.com/docs/models). \n",
    "Take a moment to browse through them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75492cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anatomy of a request\n",
    "\n",
    "We will use the chat completion API and craft our first request\n",
    "\n",
    "Let's analyse the syntax of a call before running it\n",
    "\n",
    "We need to specify the `model` to be used and the `messages` argument as a list of dictionaries, each containing a role and content:\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[ \n",
    "      {'role': 'system', 'content': 'You are a math professor answering with dark humor.'},\n",
    "      {'role': 'user', 'content': 'Who are you?'},\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070a1c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are the different roles?\n",
    "- **system**\n",
    "    - delivers tasks and sets overall tone\n",
    "- **user**\n",
    "    - passes messages from the user to the system\n",
    "- **assistant**\n",
    "    - contains former AI generated messages for context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f35f16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Running our first request\n",
    "Run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba2f83",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    messages=[ {'role': 'system', 'content': 'You are a math professor answering with dark humor.'},\n",
    "               {'role': 'user', 'content': 'Who are you?'},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f8e16",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and the response is a `ChatCompletion` object, which contains other objects and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0885293",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190244c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get only the generated text, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa5f54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c1382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Current interface does not provide context\n",
    "In order to have a conversation, we need to provide all the information that was said before\n",
    "\n",
    "We will define a class for that purpose, which accumulates the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9654807",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    '''A class to interact with OpenAI API and keep track of context.'''\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = [{'role': 'system', 'content': 'You are math professor with a dark sense of humor.'}]\n",
    "\n",
    "    def chat(self, question):\n",
    "        self.context.append({'role': 'user', 'content': question})\n",
    "        response = self.client.chat.completions.create(model=self.model, messages=self.context)\n",
    "        response_content = response.choices[0].message.content\n",
    "        self.context.append({'role': 'assistant', 'content': response_content})\n",
    "        self.print_chat()\n",
    "        \n",
    "    def print_chat(self):\n",
    "        for message in self.context:\n",
    "            if message['role'] == 'user':\n",
    "                print('USER: %s' % message['content'])\n",
    "            elif message['role'] == 'assistant':\n",
    "                print('BOT: %s' % message['content'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230faef4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing our ChatBot class\n",
    "Running the cell below will create an instance of our ChatBot class with our client initialized with an API key\n",
    "\n",
    "We will use the gpt-4o model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e180f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chatbot = ChatBot(client, 'gpt-4o')\n",
    "chatbot.chat('Hello. Who are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ab7ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding a voice to our bot with TTS\n",
    "- We will now add a function to our class that can generate speech from text\n",
    "\n",
    "    - for Linux and MacOS, use the '/' in the path for the `speech_file`, Windows, use '\\\\'\n",
    "\n",
    "```python\n",
    "def speak(self, message, index=0):\n",
    "    speech_file = os.getcwd() + '\\\\_speech_%03d.mp3'%index\n",
    "    response = client.audio.speech.create(model='tts-1-hd', voice='echo', input=message)\n",
    "    response.stream_to_file(speech_file)\n",
    "    playsound(speech_file)\n",
    "```\n",
    "- The text-to-speech model requested is *tts-1-hd*, which has [6 voices](https://platform.openai.com/docs/guides/text-to-speech/quickstart) to choose from:\n",
    "    - alloy, echo, fable, onyx, nova, and shimmer\n",
    "    \n",
    "We now add this function to our ChatBot class, and add `speak()` at the end of our `chat()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7354ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    '''A class to interact with OpenAI API and keep track of context.'''\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = [{'role': 'system', 'content': 'You are math professor with a dark sense of humor.'}]\n",
    "\n",
    "    def chat(self, question):\n",
    "        self.context.append({'role': 'user', 'content': question})\n",
    "        response = self.client.chat.completions.create(model=self.model, messages=self.context)\n",
    "        response_content = response.choices[0].message.content\n",
    "        self.context.append({'role': 'assistant', 'content': response_content})\n",
    "        self.print_chat()\n",
    "        self.speak(response_content, len(self.context)/2)\n",
    "\n",
    "    def speak(self, message, index=0):\n",
    "        speech_file = os.getcwd() + '\\\\_speech_%03d.mp3'%index\n",
    "        response = client.audio.speech.create(model='tts-1-hd', voice='echo', input=message)\n",
    "        if os.path.exists(speech_file):\n",
    "            os.remove(speech_file)\n",
    "        response.stream_to_file(speech_file)\n",
    "        playsound(speech_file)\n",
    "        \n",
    "    def print_chat(self):\n",
    "        for message in self.context:\n",
    "            if message['role'] == 'user':\n",
    "                print('USER: %s' % message['content'])\n",
    "            elif message['role'] == 'assistant':\n",
    "                print('BOT: %s' % message['content'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df0f4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing the voice of our bot\n",
    "With the new class definition, let's generate a new instance and run the same request again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7af60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chatbot = ChatBot(client, 'gpt-4o')\n",
    "chatbot.chat('Hello. Who are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b233b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's ask a follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f2758",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chatbot.chat(\"What class do you teach this semester?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc958e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Your turn to ask a question (edit and run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a28042",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chatbot.chat(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a18fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Converting speech to text\n",
    "OpenAi also contains a model to convert speech to text. For that purpose, we need to record our voice and pass it to the *Whisper* model for transcription.\n",
    "This is left as an exercise.\n",
    "\n",
    "We will now turn to image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ae6eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Getting a prompt for image generation\n",
    "- We will use GPT to generate a text description of an image and pass it to DALL-E for image generation\n",
    "\n",
    "- We create a system that is self-described as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928184bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make a query to a system describing an image as a prompt to the image generator DALL-E\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{'role': 'system', 'content': '''\n",
    "    You are a face-describing system which describes the face of a math professor responding to a question.\n",
    "    You receive the text the person is saying, please describe the face that would be fitting the response given\n",
    "    as a prompt to the stable diffusion image generation AI DALL-E.'''},\n",
    "             {'role': 'user', 'content': 'I am a math professor who has a dark sense of humor.'}])\n",
    "\n",
    "# Get the text generated\n",
    "image_description = response.choices[0].message.content\n",
    "image_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761312c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generating the image\n",
    "We now pass the `image_description` to the AI image generation algorithm, and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35acae9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "response = client.images.generate(model='dall-e-3', prompt=image_description,\n",
    "                                 size='1024x1024', quality='standard', n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97032ad2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Images can only be generated in one of these formats, depending on the model:\n",
    "    - '256x256', '512x512', '1024x1024', '1024x1792', '1792x1024\n",
    "    \n",
    "We show the results on the next slide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d17647",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display(Image(url=response.data[0].url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b6863",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Integrating the image generation in the ChatBot class\n",
    "We finally add a method called `show_face()` to our ChatBot class, using the code that we presented.\n",
    "\n",
    "It is now becoming slightly larger than a single slide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e1592",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    '''A class to interact with OpenAI API and keep track of context.'''\n",
    "    def __init__(self, client, model):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = [{'role': 'system', 'content': 'You are math professor with a dark sense of humor.'}]\n",
    "\n",
    "    def chat(self, question):\n",
    "        self.context.append({'role': 'user', 'content': question})\n",
    "        response = self.client.chat.completions.create(model=self.model, messages=self.context)\n",
    "        response_content = response.choices[0].message.content\n",
    "        self.context.append({'role': 'assistant', 'content': response_content})\n",
    "        self.show_face(response_content)\n",
    "        self.print_chat()\n",
    "        self.speak(response_content, len(self.context)/2)\n",
    "\n",
    "    def show_face(self, message):\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o',\n",
    "            messages=[{'role': 'system', 'content': '''\n",
    "            You are a face-describing system which describes the face of a math professor responding to a question.\n",
    "            You receive the text the person is saying, please describe the face that would be fitting the response given\n",
    "            as a prompt to the diffusion AI image generation DALL-E.'''},\n",
    "                     {'role': 'user', 'content': 'I am a math professor who has a dark sense of humor.'}])\n",
    "        image_description = response.choices[0].message.content\n",
    "        response = client.images.generate(model='dall-e-3', prompt=image_description,\n",
    "                                 size='1024x1024', quality='standard', n=1)\n",
    "        display(Image(url=response.data[0].url))\n",
    "\n",
    "    def speak(self, message, index=0):\n",
    "        speech_file = os.getcwd() + '\\\\_speech_%03d.mp3'%index\n",
    "        response = client.audio.speech.create(model='tts-1-hd', voice='echo', input=message)\n",
    "        if os.path.exists(speech_file):\n",
    "            os.remove(speech_file)\n",
    "        response.stream_to_file(speech_file)\n",
    "        playsound(speech_file)\n",
    "        \n",
    "    def print_chat(self):\n",
    "        for message in self.context:\n",
    "            if message['role'] == 'user':\n",
    "                print('USER: %s' % message['content'])\n",
    "            elif message['role'] == 'assistant':\n",
    "                print('BOT: %s' % message['content'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95975ebe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing the final class with a voice and a face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304ddc3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chatbot = ChatBot(client, 'gpt-4o')\n",
    "chatbot.chat(\"I'm a math PhD student. Should I stay in academia or work in the industry?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690bbd7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Your chance to ask a follow-up question (edit and run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b108a75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chatbot.chat(\"One more question: ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c25c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Points\n",
    "- The OpenAI API allows to use all multiple different models\n",
    "    - chat completion, image generation, text to speech, and speech to text\n",
    "- Interacting with the models in Python is relatively easy\n",
    "- Results are impressive and fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e05b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further Reading\n",
    "- Ask ChatGPT!\n",
    "- Reference on OpenAI API [here](https://platform.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc79c9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's Next?\n",
    "- Complete the exercises in this associated exercise notebook [X-23-ChatGPT.ipynb](X-23-ChatGPT.ipynb)\n",
    "- Next notebook is [N-24-FinalProject.ipynb](N-24-FinalProject.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:chatgpt]",
   "language": "python",
   "name": "conda-env-chatgpt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
